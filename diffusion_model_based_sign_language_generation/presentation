DDPM 논문의 Key Advantages와 수학적 근거
1. High-Quality Generation (고품질 생성)
    설명: DDPM(Denoising Diffusion Probabilistic Models)은 점진적으로 노이즈를 제거하는 과정을 통해 매우 사실적인 이미지를 생성합니다.
    수학적 원리: 생성 과정은 정규분포에서 시작해, 수백 단계에 걸쳐 노이즈를 점진적으로 제거하며 원본 데이터 분포로 복원합니다. 각 단계는 Markov chain의 조건부 확률 분포로 모델링되며, 실제 데이터와의 차이를 최소화하는 MSE(Mean Squared Error) loss로 학습됩니다.
근거: 이 방식은 GAN과 달리 모드 붕괴(mode collapse) 없이 다양한 이미지를 생성할 수 있고, 실제 데이터 분포를 더 잘 근사합니다.

2. Training Stability (학습 안정성)
    설명: DDPM은 적대적 학습(adversarial training)이 필요 없어, GAN 대비 학습이 매우 안정적입니다.
    수학적 원리: 각 단계별로 노이즈 예측 문제를 풀며, 단일 목적 함수(MSE loss)로 최적화합니다. 이로 인해 불안정한 경쟁 구조(GAN의 discriminator-generator) 없이도 수렴이 잘 됩니다.
    근거: 학습 과정이 단순하고, loss landscape가 부드럽기 때문에 발산이나 불안정 현상이 적습니다.

3. Theoretical Rigor (이론적 엄밀성)
    설명: DDPM은 확률론적 모델링과 변분 추론(variational inference)에 기반하여 강력한 수학적 토대를 가집니다.
    수학적 원리: 데이터에 점진적으로 노이즈를 추가하는 forward process와, 이를 역전시키는 reverse process(조건부 확률분포)를 명확하게 정의합니다. 이 과정은 Ornstein-Uhlenbeck SDE(확률 미분 방정식)와 variational lower bound(ELBO)로 설명됩니다.
    근거: 각 단계의 확률분포는 명확히 정의되어 있고, 전체 모델은 변분 추론의 관점에서 최적화됩니다.

4. Controllability (생성 과정의 제어 용이성)
    설명: 생성 과정이 단계별로 진행되므로, 중간 단계에서 조건을 추가하거나, 생성 과정을 세밀하게 조정할 수 있습니다.
    수학적 원리: 각 timestep의 중간 결과에 조건(conditioning)을 추가할 수 있으며, reverse process의 각 단계에 제약을 걸어 원하는 특성을 반영할 수 있습니다.
    근거: trajectory control, 조건부 샘플링 등 다양한 제어가 자연스럽게 가능하며, 이는 GAN 등 다른 생성 모델에 비해 큰 장점입니다.

5. Flexibility (유연성)
    설명: DDPM은 이미지뿐 아니라 오디오, 텍스트 등 다양한 데이터 타입에 적용할 수 있습니다.
    수학적 원리: forward/reverse process가 확률적 노이즈 추가/제거라는 일반적인 연산에 기반해 있어, 데이터의 구조와 무관하게 적용 가능합니다. SDE 기반 수식은 데이터 타입에 맞게 변형할 수 있습니다.
    근거: 복잡한 데이터 분포에도 쉽게 적응하며, 다양한 분야에서 성공적으로 활용되고 있습니다.

요약:
DDPM은 수학적으로 엄밀한 확률적 모델링과 단계적 노이즈 제거 과정을 통해, 고품질의 다양한 데이터를 안정적으로 생성할 수 있으며, 이 과정의 제어와 확장이 매우 용이하다는 장점을 갖습니다. 이러한 특성은 GAN 등 기존 생성 모델과 차별화되는 핵심 강점입니다.

---

dz 가우시안 노이즈를 추가했다가, 이를 제거하는 방식으로 학습시킨다..

어떻게? -> 우선 가정

노이즈가 정규분포..?

---




노이즈 스케쥴 = 각 단계별로 추가할 가우시안 노이즈의 분산을 미리 정해놓은 값들의 리스트(혹은 함수)



노이즈를 너무 빨리 많이 넣으면 데이터 구조가 너무 빨리 망가져서 복원이 어려워지고,

너무 천천히 넣으면 학습이 비효율적이거나, 생성 이미지 품질이 떨어질 수 있습니다. -> 노이즈를 학습해야 하는데,,, 노이즈가 학습되지 않음 + 노이즈가 충분히 추가되지 않으면, 모델이 다양한 잠재 공간(latent space)을 탐색하지 못해, 생성 샘플의 다양성이 현저히 감소합니다.

그래서 linear, cosine, sigmoid 등 다양한 스케쥴이 연구되고 있으며, 데이터 특성에 따라 최적의 스케쥴을 선택하는 것이 중요합니다
.




일반적으로,, 디퓨전 초기에는 선형 증가로,,, 
초기값(β₁): 약 0.0001
최종값(β_T): 약 0.02
으로 했음


---

$$
x_t = \sqrt{\bar{\alpha}_t}x_0 + \sqrt{1-\bar{\alpha}_t}\epsilon, \quad \epsilon \sim \mathcal{N}(0,I)
$$

한마디로 ε은 '노이즈(noise)' 그 자체입니다. 더 정확히는 표준 정규 분포(Standard Normal Distribution)를 따르는 순수한 랜덤 노이즈를 의미합니다.

중요한 점은 ε은 데이터 x_0와 동일한 차원(shape)을 가진다는 것입니다. 예를 들어 x_0가 28x28 크기의 이미지라면, ε 역시 28x28 크기의 텐서(행렬)이며, 각 픽셀 위치의 값은 표준 정규 분포에서 독립적으로 샘플링된 랜덤한 값입니다.

---

B에 제곱을 붙이는 이유

$$
q(x_t | x_{t-1}) = \mathcal{N}(x_t; \sqrt{1-\beta_t}x_{t-1}, \beta_t I)
$$

$$
Var(ax) = a^2Var(x)
$$
$$  
Var(x + y) = Var(x) + Var(y)
$$
---
$$
x_t = \sqrt{1-B_t}x_{t-1} + \sqrt{B_t}e
$$
$$
Var(x_t) = (1-βt)Var(x_{t-1}) + B_tVar(e)
$$
---
$$
Var(\sqrt{1-B_t}x_t + B_t) = Var(\sqrt{1-B_t}x_t) + Var(B_t)
$$

$$
Var(\sqrt{1-B_t}x_t + B_t) = Var(\sqrt{1-B_t}x_t) + Var(\sqrt{B_t} + e)
$$
